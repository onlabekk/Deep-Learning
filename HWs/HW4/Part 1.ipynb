{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Part 1.ipynb","provenance":[],"collapsed_sections":["h2JmwQyTj7NX"]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"rwB2kikJj7L8"},"source":["# Homework 4\n","## Part 1 *(8 points)*\n","### POS-tagging with Transformers"]},{"cell_type":"markdown","metadata":{"id":"kTHDDISej7L-"},"source":["**Part-of-speech (POS) tagging** is the process of marking up a word in a text as corresponding to a particular part of speech (noun, verb, ...), based on both its definition and its context.\n","\n","\n","<img src=\"https://raw.githubusercontent.com/karfly/learning-deep-learning/master/09_transformer/static/pos-tagging.png\" width=480>\n","\n","Let's look at some basic examples:\n","\n","\n","**Example 1.**\n","\n","$$\\begin{align*}\n","\\text{Input} &= [\\text{I love my cat}]\\\\\n","\\text{Output} &= [\\text{PRON VERB PRON NOUN}]\n","\\end{align*}$$\n","\n","\n","**Example 2.**\n","\n","$$\\begin{align*}\n","\\text{Input} &= [\\text{august 11 , 2000}]\\\\\n","\\text{Output} &= [\\text{PROPN NUM PUNCT NUM}]\n","\\end{align*}$$\n","\n","\n","---\n","At first glance it seems that the POS-tagging problem can be solved by collecting a dictionary `{word : POS-tag}`. But natural language is very complex and usually a word's part of speech strongly depends on the context, like in the following examples: \n","\n","\n","**Example 3.**\n","\n","$$\\begin{align*}\n","\\text{Input} &= [\\text{Look , there is a } \\textbf{bear}]\\\\\n","\\text{Output} &= [\\text{VERB PUNCT DET VERB DET } \\textbf{NOUN}]\n","\\end{align*}$$\n","\n","\n","\n","**Example 4.**\n","\n","$$\\begin{align*}\n","\\text{Input} &= [\\text{I can not } \\textbf{bear } \\text{it anymore}]\\\\\n","\\text{Output} &= [\\text{PROPN VERB ADV } \\textbf{VERB } \\text{PROPN ADV}]\n","\\end{align*}$$\n","\n","In the example 3 the word `bear` is a NOUN, but `bear` in the example 4 is a VERB. This can only be derived from the context. Such examples are not rare, on the contrary, they are very common in human languages (e.g. see [homonyms](https://en.wikipedia.org/wiki/Homonym)). Today we'll tackle this problem by using the most successful (so far) deep learning architecture for NLP – [Transformer](https://arxiv.org/abs/1706.03762)!"]},{"cell_type":"markdown","metadata":{"id":"Y9y5LyIyj7MB"},"source":["### Homework plan\n","\n","In this homework you'll implement and train a **Transformer** for POS-tagging. The **main goal** of this homework is to get to know with the **Transformer architecture** and to understand how it works from the inside.\n","\n","### Grading\n","The main metric in this homework is accuracy calculated on the **test** set (please, don't train on test set – it will be easily revealed by the TAs). Also, you are not allowed to change train/val/test data.\n","\n","We will grade implementation, training and evaluation of your Transformer model. The test accuracy must be **> 85%**.\n","\n","Sorry, there are no strict grading criteria, it's very hard to come up with them. We'll rely on common sense.\n","\n","### Tips\n","- Do not start writing code before you have read and understood the theory.\n","- Read all the text in this notebook (hope, you're reading this).\n"," \n","### Disclaimer\n","ConvNets can be better than Transformers in POS-tagging task in the case of small data. In this homework you'll train models on a very small dataset (12 543 train sentences) which is ridiculous on the scale of modern NLP datasets (millions or even billions of sentences!). Anyway, it's very important to understand how Transformers work and to be ready to apply them in a real-world problem with a large dataset, where Transformers outperform other architectures (Conv, RNN, ...) by a large margin.\n","\n","**Good luck and have fun!**"]},{"cell_type":"code","metadata":{"id":"MsEbFDMnj7ML"},"source":["import os\n","import random\n","\n","import numpy as np\n","\n","import torch\n","from torch import nn\n","\n","import torchtext\n","import torchtext.datasets\n","import torchtext.data"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_NyFdymMj7MR"},"source":["device = 'cuda'\n","assert torch.cuda.is_available()\n","\n","BATCH_SIZE = 128\n","MAX_LENGTH = 200  # maximum length of the input sentence\n","MIN_TOKEN_FREQUENCY = 10  # minimum occurrence frequency"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BW9oWRR9j7MV"},"source":["seed = 0\n","\n","random.seed(seed)\n","np.random.seed(seed)\n","torch.manual_seed(seed)\n","torch.backends.cudnn.deterministic = True"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"b7ZyUxHwj7Md"},"source":["## Load and prepare data"]},{"cell_type":"markdown","metadata":{"id":"lVF9TeL4j7Me"},"source":["In this homework you'll use `UDPOS` dataset for POS-tagging. In fact, `UDPOS` dataset is just a collection of pairs `(sequence of words : sequence of corresponding POS-tags)`.\n","\n","\n","For easy access to the dataset, use [torchtext](https://pytorch.org/text/) library. At first glance, [torchtext](https://pytorch.org/text/)'s interfaces look very strange, but after multiple uses you get used to it, and it appears to be a very nice library.\n","\n","**Note**: `torchtext.legacy.*` is obsolete (hence the name) and is here just for the educational purpose; it will be removed in the next edition of this homework. Don't use it in real projects.\n","\n","Let's define `fields` ([torchtext](https://pytorch.org/text/) abstraction) for text and POS-tags:"]},{"cell_type":"code","metadata":{"id":"nkAJLFTAj7Me"},"source":["TEXT = torchtext.legacy.data.Field(\n","    batch_first=True,\n","    lower=True,  # make lowercase\n","    init_token='<bos>', eos_token='<eos>'  # special tokens: beginning/end of sequence\n",")\n","\n","POSTAG = torchtext.legacy.data.Field(\n","    batch_first=True,\n","    init_token='<bos>', eos_token='<eos>'\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XZfDRnEnj7Mg"},"source":["Now let's get train/val/test data using defined fields:"]},{"cell_type":"code","metadata":{"id":"4ztCcdycj7Mg","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619516345270,"user_tz":-180,"elapsed":1924,"user":{"displayName":"Egor Burkov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GivDVP65Q7sIlhqFghmQRlwLNvSizgq3wOE8-mENQ=s64","userId":"02986759743439025897"}},"outputId":"662a4ca9-63e1-4f4d-b697-fa1f6fe1b89d"},"source":["train_data, val_data, test_data = torchtext.legacy.datasets.UDPOS.splits(\n","    fields=(('text', TEXT), ('postag', POSTAG))\n",")\n","\n","print(f\"Train size: {len(train_data)} sequences\")\n","print(f\"Val size: {len(val_data)} sequences\")\n","print(f\"Test size: {len(test_data)} sequences\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Train size: 12543 sequences\n","Val size: 2002 sequences\n","Test size: 2077 sequences\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Js_muyqKj7Mi"},"source":["Then we build the vocabulary (*note:* using only train data) to get the mapping from token to some unique index:"]},{"cell_type":"code","metadata":{"id":"FZxJWm3Mj7Mi"},"source":["TEXT.build_vocab(train_data.text, min_freq=MIN_TOKEN_FREQUENCY)  # filter out rarely occured tokens\n","POSTAG.build_vocab(train_data.postag)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_ZdtCWW0j7Mk"},"source":["To convert from token to unique index use `.vocab.stoi[...]` method (`.vocab.itos[...]` for inverse mapping). Here we collect unique indices of `padding` special tokens to use them later:"]},{"cell_type":"code","metadata":{"id":"DBImYsXpj7Mk","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619516349427,"user_tz":-180,"elapsed":727,"user":{"displayName":"Egor Burkov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GivDVP65Q7sIlhqFghmQRlwLNvSizgq3wOE8-mENQ=s64","userId":"02986759743439025897"}},"outputId":"056dc5ef-7bef-4165-e991-076e1436e0e7"},"source":["TEXT_PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n","POSTAG_PAD_IDX = POSTAG.vocab.stoi[POSTAG.pad_token]\n","print(f\"TEXT_PAD_IDX={TEXT_PAD_IDX}, POSTAG_PAD_IDX={POSTAG_PAD_IDX}\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["TEXT_PAD_IDX=1, POSTAG_PAD_IDX=1\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"DOHHhes2j7Mm"},"source":["Get vocabulary sizes:"]},{"cell_type":"code","metadata":{"id":"4lUE7iSQj7Mm","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619516350510,"user_tz":-180,"elapsed":847,"user":{"displayName":"Egor Burkov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GivDVP65Q7sIlhqFghmQRlwLNvSizgq3wOE8-mENQ=s64","userId":"02986759743439025897"}},"outputId":"5b0cc48f-8b7a-47e3-82ce-27a843f50cfe"},"source":["INPUT_DIM = len(TEXT.vocab)\n","OUTPUT_DIM = len(POSTAG.vocab)\n","print(f\"Number of unique words: {INPUT_DIM}\")\n","print(f\"Number of unique POS-tags: {OUTPUT_DIM}\")\n","print(\"All POS-tags:\", ' '.join(POSTAG.vocab.itos))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Number of unique words: 2139\n","Number of unique POS-tags: 21\n","All POS-tags: <unk> <pad> <bos> <eos> NOUN PUNCT VERB PRON ADP DET PROPN ADJ AUX ADV CCONJ PART NUM SCONJ X INTJ SYM\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"p_Hy9_ccj7Mq"},"source":["Finally, we define the data iterators that we'll use for training and evaluation:"]},{"cell_type":"code","metadata":{"id":"hWQPoRHUj7Mq"},"source":["train_iterator, val_iterator, test_iterator = torchtext.legacy.data.BucketIterator.splits(\n","    (train_data, val_data, test_data), \n","     batch_size=BATCH_SIZE,\n","     device=device\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JQhCA_g3j7Ms"},"source":["Let's grab a batch and look inside:"]},{"cell_type":"code","metadata":{"id":"rEPENhfdj7Mt","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619516363009,"user_tz":-180,"elapsed":10249,"user":{"displayName":"Egor Burkov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GivDVP65Q7sIlhqFghmQRlwLNvSizgq3wOE8-mENQ=s64","userId":"02986759743439025897"}},"outputId":"12b4e370-9502-43bd-8ae9-ef966a64741e"},"source":["batch = next(iter(train_iterator))\n","text, postag = batch.text, batch.postag\n","\n","print(f\"text.shape = {text.shape} = [batch_size, seq_len]\")\n","print(f\"postag.shape = {postag.shape} = [batch_size, seq_len]\")\n","print(f\"Datatype is {text.dtype}\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["text.shape = torch.Size([128, 50]) = [batch_size, seq_len]\n","postag.shape = torch.Size([128, 50]) = [batch_size, seq_len]\n","Datatype is torch.int64\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"eCbUXQufj7Mv"},"source":["Well, that's all about the data!"]},{"cell_type":"markdown","metadata":{"id":"_jGN_S7mj7NE"},"source":["## Transformer\n","\n","In this part you will implement a slightly modified version of the Transformer model from the [Attention is All You Need](https://arxiv.org/abs/1706.03762) paper. Actually, you'll only implement the encoder part, because it's enough for POS-tagging task.\n","\n","It's **highly recommended** to read [the original Transformer paper](https://arxiv.org/abs/1706.03762) or [this nice article](https://jalammar.github.io/illustrated-transformer/) before starting coding.\n","\n","The Transformer consists of multiple nested modules like a matryoshka (e.g. `Encoder` consists of `EncoderLayer`, which consists of `MultiHeadAttentionLayer` and `PositionwiseFeedforwardLayer`). In this notebook the implementation of modules is ordered in a [top-down](https://en.wikipedia.org/wiki/Top-down_and_bottom-up_design) manner – first the higher-level modules are implemented (e.g. `Encoder`), that use lower-level modules (e.g. `EncoderLayer`), which are not implemented yet. It's **strongly recommended** to read the notebook to the end to be aware of the code structure, before you start implementing the Transformer.\n","\n","![](https://raw.githubusercontent.com/karfly/learning-deep-learning/master/09_transformer/static/transformer.png)\n","\n","The Transformer does not use any recurrent relations. It also does not use any convolutional layers. Instead the model is entirely made up of linear layers, **attention mechanisms** and normalizations.\n","\n","As of 2021, Transformers are the dominant architecture in NLP and are used to achieve state-of-the-art results for many tasks and it appears that they will be in the near future. \n","\n","\n","*Note:* in this notebook you'll implement a **learned positional encoding** (in the fashion of [BERT](https://arxiv.org/abs/1810.04805)), not the static one from the [original paper](https://arxiv.org/abs/1706.03762)."]},{"cell_type":"markdown","metadata":{"id":"BVIDC_vuj7NE"},"source":["### Encoder\n","\n","The Transformer's encoder attempts to *transform* the entire source sentence, $X = (x_1, ... ,x_n)$, into a sequence of context vectors, $Z = (z_1, ... , z_n)$. So, if our input sequence was 5 tokens long we would have $Z = (z_1, z_2, z_3, z_4, z_5)$. Why do we call this a sequence of context vectors and not a sequence of hidden states? A hidden state at time $t$ in an RNN has only seen tokens $x_t$ and all the tokens before it. However, each context vector here has seen all tokens at all positions within the input sequence.\n","\n","![](https://raw.githubusercontent.com/karfly/learning-deep-learning/master/09_transformer/static/transformer-encoder.png)\n","\n","First, the tokens are passed through a standard embedding layer. Next, as the model has no recurrence it has no idea about the order of the tokens within the sequence. We solve this by using a second embedding layer called a *positional embedding layer*. This is a standard embedding layer where the input is not the token itself but the position of the token within the sequence, starting with the first token, the `<bos>` (beginning of sequence) token, in position 0. The position embedding has a \"vocabulary\" size of `max_length`, which means our model can accept sentences up to `max_length` tokens long.\n","\n","The original Transformer implementation from the [Attention is All You Need](https://arxiv.org/abs/1706.03762) paper does not learn positional embeddings. Instead it uses a fixed static embedding. Modern Transformer architectures, like [BERT](https://arxiv.org/abs/1810.04805), use positional embeddings instead, so we'll use them.\n","\n","Next, the token and positional embeddings are elementwise summed together to get a vector which contains information about the token and also its position within the sequence. ~~However, before they are summed, the token embeddings are multiplied by a scaling factor which is $\\sqrt{d_{model}}$, where $d_{model}$ is the hidden dimension size, `hid_dim`. This (supposedly) reduces variance in the embeddings and the model is difficult to train reliably without this scaling factor.~~ (*crossed out because [it was misleading](https://deeplearnings-3fl6116.slack.com/archives/C01SF4PKJG6/p1620322986136200)*) To train the model reliably, it's important to make sure that (1) these two summands have values of roughly the same scale so that none of them outpowers the other, and that (2) the sum is roughly within the range that your next layer is initialized for (usually around -2 to 2). Dropout is then applied to the combined embeddings.\n","\n","The combined embeddings are then passed through $N$ *encoder layers* to get $Z$, which is the output and can be used for any downstream task (e.g. POS-tagging).\n","\n","The mask, `mask`, is simply the same shape as the source sentence but has a value of 1 when the token in the source sentence is not a `<pad>` token and 0 when it is a `<pad>` token. This is used in the encoder layers to mask the multi-head attention mechanisms, which are used to calculate and apply attention over the source sentence, so the model does not pay attention to `<pad>` tokens, which contain no useful information."]},{"cell_type":"code","metadata":{"id":"-6mcd0H3j7NF"},"source":["class Encoder(nn.Module):\n","    def __init__(self,\n","                 input_dim, \n","                 hid_dim, \n","                 n_layers, \n","                 n_heads, \n","                 pf_dim,\n","                 dropout, \n","                 padding_index=None,\n","                 max_length=128):\n","        super().__init__()\n","        \n","        self.padding_index = padding_index  # if None, don't use masking\n","        \n","        # embeddings\n","        self.tok_embedding = nn.Embedding(input_dim, hid_dim)\n","        self.pos_embedding = nn.Embedding(max_length, hid_dim)\n","        \n","        # encoder layers (implemented below)\n","        self.layers = nn.ModuleList([EncoderLayer(hid_dim, n_heads, pf_dim, dropout) for _ in range(n_layers)])\n","        \n","        # dropout is applied after summing up token and positional embeddings\n","        self.dropout = nn.Dropout(dropout)\n","        \n","        # custom weight initialization\n","        self.init_weights()\n","        \n","    def init_weights(self):\n","        for m in self.modules():\n","            if hasattr(m, 'weight') and m.weight.dim() > 1:\n","                nn.init.xavier_uniform_(m.weight.data)\n","        \n","    def forward(self, x):\n","        \"\"\"\n","        x (batch of token indices): torch.long tensor of shape [bs, seq_len]\n","        \n","        returns (encoded sequence): torch.float32 tensor of shape [bs, seq_len, output_dim]\n","        \"\"\"\n","        device = x.device\n","        bs, seq_len = x.shape[:2]\n","        \n","        # compute non-padding mask (use self.padding_index)\n","        mask = None\n","        if self.padding_index is not None:\n","            mask = ## your code here\n","        \n","        # get token embeddings and scale with self.scale parameter\n","        ## your code here\n","        \n","        # generate input [0, 1, ..., seq_len - 1] for positional embedder [bs, seq_len]\n","        ## your code here\n","        \n","        # get pos embeddings\n","        ## your code here\n","        \n","        # sum up token and positional embeddings\n","        ## your code here\n","        \n","        # apply dropout\n","        ## your code here\n","        \n","        # apply encoder layers one by one; input shape is [bs, seq_len, hid dim]\n","        ## your code here\n","        \n","        return x"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LAzyAzF4j7NG"},"source":["### Encoder Layer\n","\n","The encoder layers are where all of the \"meat\" of the encoder is contained.\n","\n","![](https://raw.githubusercontent.com/karfly/learning-deep-learning/master/09_transformer/static/transformer-encoder-layer.png)\n","\n","The encoder layer consists of 2 main blocks:\n","1. Pass the source sentence and its mask into the *multi-head attention layer*, perform dropout on it, add a residual connection and pass it through a [Layer Normalization](https://arxiv.org/abs/1607.06450) layer.\n","2. Pass the output of the 1st block through a *position-wise feedforward* layer and then, again, apply dropout, a residual connection and then layer normalization to get the output of this layer which is fed into the next layer (the parameters are not shared between layers)\n","\n","\n","The multi-head attention layer is used by the encoder layer to attend to the source sentence, i.e. it is calculating and applying attention over itself instead of another sequence, hence we call it *self attention*.\n","\n","[This article](https://mlexplained.com/2018/01/13/weight-normalization-and-layer-normalization-explained-normalization-in-deep-learning-part-2/) goes into more detail about layer normalization, but the gist is that it normalizes the values of the features, i.e. across the hidden dimension, so each feature has a mean of 0 and a standard deviation of 1. This allows neural networks with a larger number of layers, like the Transformer, to be trained easier."]},{"cell_type":"code","metadata":{"id":"NxE34atwj7NG"},"source":["class EncoderLayer(nn.Module):\n","    def __init__(self, \n","                 hid_dim, \n","                 n_heads, \n","                 pf_dim,  \n","                 dropout):\n","        super().__init__()\n","        \n","        # self-attention layer normalization\n","        self.attention_layer_norm = nn.LayerNorm(hid_dim)\n","        \n","        # positionwise feedforward layer normalization\n","        self.pf_layer_norm = nn.LayerNorm(hid_dim)\n","        \n","        # attention layer (implemented below)\n","        self.attention = MultiHeadAttentionLayer(hid_dim, n_heads, dropout)\n","        \n","        # positionwise feedforward layer (implemented below)\n","        self.pf = PositionwiseFeedforwardLayer(hid_dim, pf_dim, dropout)\n","        \n","        # dropout is applied to the outputs of the attention and positionwise feedforward layers\n","        self.dropout = nn.Dropout(dropout)\n","        \n","    def forward(self, x, mask=None):\n","        \"\"\"\n","        x (sequence of vectors): torch.float32 tensor of shape [bs, seq_len, hid_dim]\n","        mask (mask of valid elements): torch.bool tensor of shape [bs, seq_len]\n","        \n","        returns (processed sequence of vectors): torch.float32 tensor of shape [bs, seq_len, hid_dim]\n","        \"\"\"\n","        ### block 1\n","        # calculate self-attention + dropout\n","        ## your code here\n","        \n","        # residual (attention) + attention layer norm\n","        ## your code here\n","        \n","        ### block 2\n","        # calculate positionwise feedforward + dropout\n","        ## your code here\n","        \n","        # residual (positionwise feedforward) + positionwise feedforward layer norm\n","        ## your code here\n","        \n","        return x"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LYf6cuVFj7NH"},"source":["### Multi-Head Attention Layer\n","\n","One of the key, novel concepts introduced by the Transformer paper is the *multi-head attention layer*. \n","\n","![](https://raw.githubusercontent.com/karfly/learning-deep-learning/master/09_transformer/static/transformer-attention.png)\n","\n","Attention can be thought of as *queries*, *keys* and *values* - where the query is used with the key to get an attention vector (usually the output of a *softmax* operation and has all values between 0 and 1 which sum to 1) which is then used to get a weighted sum of the values.\n","\n","The Transformer uses *scaled dot-product attention*, where the query and the key are combined by taking the dot product between them, then applying the softmax operation, scaling by $d_k$ and finally multiplying by the value. $d_k$ is the *head dimension*, `head_dim`, which we will shortly explain further.\n","\n","$$ \\text{Attention}(Q, K, V) = \\text{Softmax} \\big( \\frac{QK^T}{\\sqrt{d_k}} \\big)V $$ \n","\n","This is similar to standard *dot product attention* but is scaled by $d_k$, which the paper states is used to stop the results of the dot products growing large, causing gradients to become too small.\n","\n","However, the scaled dot-product attention isn't simply applied to the queries, keys and values. Instead of doing a single attention application the queries, keys and values have their `hid_dim` split into $h$ *heads* and the scaled dot-product attention is calculated over all heads in parallel. This means instead of paying attention to one concept per attention application, we pay attention to $h$. We then re-combine the heads into their `hid_dim` shape, thus each `hid_dim` is potentially paying attention to $h$ different concepts.\n","\n","$$ \\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1,...,\\text{head}_h)W^O $$\n","\n","$$\\text{head}_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V) $$\n","\n","$W^O$ is the linear layer applied at the end of the multi-head attention layer, `fc`. $W^Q, W^K, W^V$ are the linear layers `fc_q`, `fc_k` and `fc_v`.\n","\n","The steps in this module are as follows:\n","1. Calculate $QW^Q$, $KW^K$ and $VW^V$ with the linear layers, `fc_q`, `fc_k` and `fc_v` to get `Q`, `K` and `V`.\n","2. Split the `hid_dim` of the query, key and value into `n_heads` (use `.view`) and correctly permute them so they can be multiplied together.\n","3. Calculate the `energy` (the un-normalized attention) by multiplying `Q` and `K` together and scaling it by the square root of `head_dim`, which is calculated as `hid_dim // n_heads`. Mask the energy not to pay attention over any elements of the sequence we shouldn't.\n","4. Apply the softmax, dropout and then apply the attention to the value heads, `V`, before combining the `n_heads` together.\n","5. Finally, multiply the output of step 4 with $W^O$ (represented as `fc_o`). \n","\n","Note that in our implementation the lengths of the keys and values are always the same, thus when matrix multiplying the output of the softmax, `attention`, with `V` we will always have valid dimension sizes for matrix multiplication. This multiplication is carried out using the `@` operator which, when both tensors are >2-dimensional, does a batched matrix multiplication over the last two dimensions of each tensor. This will be a **[query len, key len] x [value len, head dim]** batched matrix multiplication over the batch size and each head which provides the **[batch size, n heads, query len, head dim]** result.\n","\n","One thing that looks strange at first is that dropout is applied directly to the attention. This means that our attention vector will most probably not sum to 1 and we may pay full attention to a token but the attention over that token is set to 0 by dropout. This is never explained, or even mentioned, in the paper however is used by the [official implementation](https://github.com/tensorflow/tensor2tensor/) and every Transformer implementation since, [including BERT](https://github.com/google-research/bert/)."]},{"cell_type":"code","metadata":{"id":"d7GejVlLj7NI"},"source":["class MultiHeadAttentionLayer(nn.Module):\n","    def __init__(self, hid_dim, n_heads, dropout):\n","        super().__init__()\n","        \n","        assert hid_dim % n_heads == 0, \"hid_dim must be divisible by n_heads\"\n","        \n","        self.hid_dim = hid_dim\n","        self.n_heads = n_heads\n","        self.head_dim = hid_dim // n_heads\n","        \n","        # query, key and value linear networks\n","        self.fc_q = nn.Linear(hid_dim, hid_dim)\n","        self.fc_k = nn.Linear(hid_dim, hid_dim)\n","        self.fc_v = nn.Linear(hid_dim, hid_dim)\n","        \n","        # output linear networks\n","        self.fc_o = nn.Linear(hid_dim, hid_dim)\n","        \n","        # dropout is applied to attention\n","        self.dropout = nn.Dropout(dropout)\n","        \n","        # scale parameter\n","        self.scale = torch.nn.Parameter(torch.sqrt(torch.tensor(self.head_dim, dtype=torch.float32)), requires_grad=False)\n","        \n","    def forward(self, query, key, value, mask=None):\n","        \"\"\"\n","        query/key/value (batch of queries/keys/values): torch.float32 tensor of shape [bs, seq_len, hid_dim]\n","        mask (mask of valid elements): torch.bool tensor of shape [bs, seq_len]\n","        \n","        returns (multi-head attention): torch.float32 tensor of shape [bs, seq_len, hid_dim]\n","        \"\"\"\n","        \n","        bs = query.shape[0]\n","        \n","        # calculate Q, K, V using corresponding linear networks\n","        q, k, v = self.fc_q(query), self.fc_k(key), self.fc_v(value)  # shape is [bs, seq_len, hid_dim]\n","                \n","        # prepare Q, K, V for the `@` operator\n","        # shape is [bs, n_heads, seq_len, head_dim]\n","        ## your code here\n","        \n","        # compute energy using the `@` operator (don't forget to scale!)\n","        # shape is [bs, n_heads, seq_len, seq_len]\n","        energy = ## your code here  \n","        \n","        # apply mask – 1 in mask is a valid element, 0 - not; fill with some large negative numbers (use .masked_fill())\n","        if mask is not None:\n","            energy = ## your code here\n","        \n","        # apply softmax along the last dim of energy and get the attention weights + dropout\n","        # shape is [bs, n_heads, seq_len, seq_len]\n","        attention = ## your code here\n","        \n","        # weight values with calculated attention (use `@` operator)\n","        # shape is [bs, n_heads, seq_len, head_dim]\n","        x = ## your code here\n","        \n","        # squash 1 and 4 dims back\n","        x = x.permute(0, 2, 1, 3).contiguous()\n","        x = x.view(bs, -1, self.hid_dim)  # shape is [bs, seq_len, hid_dim]\n","        \n","        # apply output linear layer\n","        x = ## your code here\n","        \n","        return x"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vWjirq-rj7NJ"},"source":["### Position-wise Feedforward Layer\n","\n","The other main block inside the encoder layer is the *position-wise feedforward layer* This is relatively simple compared to the multi-head attention layer. The input is transformed from `hid_dim` to `pf_dim`, where `pf_dim` is usually a lot larger than `hid_dim`. The original Transformer used a `hid_dim` of 512 and a `pf_dim` of 2048. The ReLU activation function and dropout are applied before it is transformed back into a `hid_dim` representation. \n","\n","Why is this used? Unfortunately, it is never explained in the paper.\n","\n","*The bonus*: we implemented this layer for you!"]},{"cell_type":"code","metadata":{"id":"7Tm1VN2Uj7NJ"},"source":["class PositionwiseFeedforwardLayer(nn.Module):\n","    def __init__(self, hid_dim, pf_dim, dropout):\n","        super().__init__()\n","        \n","        # linear layers\n","        self.fc_1 = nn.Linear(hid_dim, pf_dim)\n","        self.fc_2 = nn.Linear(pf_dim, hid_dim)\n","        \n","        # dropout is applied after the first layer\n","        self.dropout = nn.Dropout(dropout)\n","        \n","    def forward(self, x):\n","        \"\"\"\n","        x (sequence of vectors): torch.float32 tensor of shape [bs, seq_len, hid_dim]\n","        \n","        returns (processed sequence of vectors): torch.float32 tensor of shape [bs, seq_len, hid_dim]\n","        \"\"\"\n","        # apply linear layers + dropout\n","        x = self.dropout(torch.relu(self.fc_1(x)))\n","        x = self.fc_2(x)\n","        \n","        return x"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nlaNASAPj7NK"},"source":["## Transformer POS-tagger"]},{"cell_type":"markdown","metadata":{"id":"PDaDuBrTj7NK"},"source":["Now we have all the parts of the transformer and we can build a Transformer-based POS-tagger. It consists of Transformer encoder and single linear layer, which predicts the classes of POS-tags:"]},{"cell_type":"code","metadata":{"id":"-fifCVq1j7NK"},"source":["class TransformerPOSTagger(nn.Module):\n","    def __init__(self,\n","                 input_dim,\n","                 output_dim,\n","                 hid_dim=64, \n","                 n_layers=8, \n","                 n_heads=8, \n","                 pf_dim=64,\n","                 dropout=0.1, \n","                 padding_index=None,\n","                 max_length=128):\n","        super().__init__()\n","        \n","        # transformer encoder\n","        self.encoder = Encoder(\n","            input_dim,\n","            hid_dim=hid_dim, \n","            n_layers=n_layers, \n","            n_heads=n_heads, \n","            pf_dim=pf_dim,\n","            dropout=dropout, \n","            padding_index=padding_index,\n","            max_length=max_length\n","        )\n","        \n","        # linear layer to predict (classify) pos-tags\n","        self.postag_predictor = nn.Linear(hid_dim, output_dim)\n","        \n","    def forward(self, x):\n","        # apply encoder\n","        ## your code here\n","        \n","        # predict postags\n","        ## your code here\n","        \n","        return x"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"faJX_6cNj7M5"},"source":["## Training Pipeline\n","\n","A common practice in training deep learning models is to a create single-function `run_epoch(...)`, which can be used both for training and evaluation (see `phase` parameter). Here you need to implement forward of the model and the loss calculation. Good news is that you'll reuse this function later for training the Transformer model."]},{"cell_type":"code","metadata":{"id":"_J-bcATu5g6z"},"source":["# If `True`, train the model.\n","# If `False`, only load weights from \"TransformerPOSTagger.pth\" and evaluate\n","DO_TRAIN = True"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SEmUDyngmuHh"},"source":["def run_epoch(model, dataloader, optimizer, criterion, phase='train'...):\n","    is_train = (phase == 'train')\n","    if is_train:\n","        model.train()\n","    else:\n","        model.eval()\n","    \n","    epoch_loss = 0\n","\n","    # variables for calculating accuracy\n","    n_predicted, n_true_predicted = 0, 0\n","    \n","    with torch.set_grad_enabled(is_train):\n","        for i, batch in enumerate(dataloader):\n","            # unpack batch\n","            text, postag = batch.text, batch.postag\n","            \n","            # make prediction\n","            \n","            # reshape prediction to [-1, output_dim]\n","            \n","            # reshape gt labels to [-1, ]\n","            \n","            # calculate loss\n","            \n","            if is_train:\n","                # make optimization step\n","                \n","            # calculate accuracy\n","            n_true_predicted += ((pred.argmax(-1) == gt) * (gt != POSTAG_PAD_IDX)).sum().item()  # exclude pad token\n","            n_predicted += torch.sum(gt != POSTAG_PAD_IDX).item()\n","            \n","            # log per-batch train metrics\n","                \n","            epoch_loss += loss.item()\n","\n","        average_loss = epoch_loss / len(dataloader)\n","        average_accuracy = n_true_predicted / n_predicted\n","        \n","        # log per-epoch metrics\n","\n","        return average_loss, average_accuracy"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EaP7Gv00j7NM"},"source":["hid_dim = ## your code here\n","n_layers = ## your code here\n","n_heads = ## your code here\n","pf_dim = ## your code here\n","dropout = ## your code here\n","\n","model = TransformerPOSTagger(\n","    INPUT_DIM,\n","    OUTPUT_DIM,\n","    hid_dim=hid_dim, \n","    n_layers=n_layers, \n","    n_heads=n_heads, \n","    pf_dim=pf_dim, \n","    dropout=dropout, \n","    padding_index=TEXT_PAD_IDX,\n","    max_length=MAX_LENGTH\n",").to(device)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RvQ9vqyyj7NN"},"source":["The learning rate needs to be lower than the default used by Adam, otherwise the learning can be unstable.\n","\n","Any loss function suitable for classification will also work for us, so we'll choose the obvious standard option – [nn.CrossEntropyLoss](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html) criterion. Make sure to ignore losses calculated over `<pad>` tokens, not to add noise to the total loss (use `ignore_index=`)."]},{"cell_type":"code","metadata":{"id":"hW65LgNBj7NN"},"source":["def count_parameters(model):\n","    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n","\n","criterion = ## ...\n","\n","if DO_TRAIN:\n","    optimizer = ## ...\n","\n","    best_model_path = ## ...\n","\n","    print(f\"Model has {count_parameters(model)} trainable parameters\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bcxp0-P2j7NO"},"source":["Train Transformer POS-tagger:"]},{"cell_type":"code","metadata":{"id":"SkeT3H_Lj7NO","scrolled":true},"source":["if DO_TRAIN:\n","    n_epochs = ## your code here\n","\n","    best_val_loss = float('+inf')\n","    for epoch in range(n_epochs):\n","        train_loss, train_accuracy = run_epoch(model, train_iterator, optimizer, criterion, phase='train')\n","        val_loss, val_accuracy = run_epoch(model, val_iterator, None, criterion, phase='val')\n","\n","        if val_loss < best_val_loss:\n","            best_val_loss = val_loss\n","            torch.save(model.state_dict(), best_model_path)\n","\n","        print(f'Epoch: {epoch+1:02}')\n","        print(f'\\tTrain Loss: {train_loss:.3f} | Train accuracy: {train_accuracy * 100:.2f}')\n","        print(f'\\t Val. Loss: {val_loss:.3f} |  Val. accuracy: {val_accuracy * 100:.2f}')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BF_Vw28Hj7NQ"},"source":["Rename the best model to `TransformerPOSTagger.pth` and upload it to Google Drive, set \"Anyone with the link can view\". Like in the assignment №2, fill the md5 checksum and the link."]},{"cell_type":"code","metadata":{"id":"Fqq7qwYrmuHk"},"source":["best_model_md5_checksum = # ...\n","best_model_gdrive_link = # ... "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Y0eHe41amuHk"},"source":["This is how we check your model when grading the assignment."]},{"cell_type":"code","metadata":{"id":"i412TCMij7NQ"},"source":["if DO_TRAIN:\n","    model.load_state_dict(torch.load(best_model_path, map_location=device))\n","else:\n","    # Download the checkpoint and initialize model weights from it\n","    import urllib\n","    import subprocess\n","\n","    penalize = False\n","\n","    # Get your link and checksum\n","    claimed_md5_checksum, google_drive_link = best_model_md5_checksum, best_model_gdrive_link\n","    \n","    WEIGHTS_FILE = \"./TransformerPOSTagger.pth\"\n","\n","    # Use your link to download \"checkpoint.pth\"\n","    !gdown --id {urllib.parse.urlparse(google_drive_link).path.split('/')[-2]}\n","\n","    try:\n","        # Compute the actual checksum\n","        real_md5_checksum = subprocess.check_output(\n","            [\"md5sum\", WEIGHTS_FILE]).decode().split()[0]\n","    except subprocess.CalledProcessError as err:\n","        # Couldn't download or the filename isn't \"TransformerPOSTagger.pth\"\n","        print(f\"Wrong link or filename: {err}\")\n","        penalize = True\n","    else:\n","        # The trained checkpoint is different from the one submitted\n","        if real_md5_checksum != claimed_md5_checksum:\n","            print(\"Checksums differ! Late submission?\")\n","            penalize = True\n","\n","    if penalize:\n","        print(\"🔫 Prepare the penalizer! 🔫\")\n","\n","    # Finally load weights\n","    model.load_state_dict(torch.load(WEIGHTS_FILE, map_location=device))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"USnMMj7DmuHk"},"source":["test_loss, test_accuracy = run_epoch(model, test_iterator, None, criterion, phase='val')\n","print(f'Test Loss: {test_loss:.3f} | Test accuracy: {test_accuracy:7.3f}')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7tuFlGFyj7M-"},"source":["Next we'll implement functions to infer our model on any given sentence and display the outputs:"]},{"cell_type":"code","metadata":{"id":"INQ12G9Vj7M-"},"source":["def infer_model(sentence, model, device):\n","    model.eval()\n","    \n","    # get tokens\n","    if isinstance(sentence, str):\n","        tokens = [token.lower() for token in sentence.split(' ')]  # split if string\n","    else:\n","        tokens = [token.lower() for token in sentence]\n","    \n","    # add <bos> and <eos> special tokens to the beginning and to the end\n","    tokens = ## your code here\n","    \n","    # convert tokens to indices\n","    indices = ## your code here\n","    indices = torch.LongTensor(indices).unsqueeze(0).to(device)\n","\n","    # make predictions\n","    with torch.no_grad():\n","        pred = ## your code here\n","        \n","    # extract pos-tags indices from predictions using .argmax(...)\n","    pred_postag_indices = ## your code here\n","    \n","    # convert from indices to pos-tags\n","    pred_postags = ## your code here\n","    \n","    # cut off <bos> and <eos>\n","    tokens = tokens[1:-1]\n","    pred_postags = pred_postags[1:-1]\n","\n","    return tokens, pred_postags\n","\n","\n","def print_predictions(tokens, pred_postags, gt_postags=None):\n","    print(\"===> Input sentence:\", ' '.join(tokens))\n","    print()\n","    \n","    if gt_postags is not None:\n","        print(\"Pred. POS-tag\\tGT POS-tag\\tCorrect?\\tToken\\n\")\n","        for token, pred_postag, gt_postag in zip(tokens, pred_postags, gt_postags):\n","            correct = '✔' if pred_postag == gt_postag else '✘'\n","            print(f\"{pred_postag}\\t\\t{gt_postag}\\t\\t{correct}\\t\\t{token}\")\n","    else:\n","        print(\"Pred. POS-tag\\tToken\\n\")\n","\n","        for token, pred_postag in zip(tokens, pred_postags):\n","            print(f\"{pred_postag}\\t\\t{token}\")\n","    print()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TIciPmAwj7M_"},"source":["Sentences from the test set:"]},{"cell_type":"code","metadata":{"id":"C_ZsZkDZj7M_"},"source":["example_index = np.random.randint(0, len(test_data.examples))\n","\n","sentence = test_data.examples[example_index].text\n","gt_postags = test_data.examples[example_index].postag\n","tokens, pred_postags = infer_model(sentence, model, device)\n","\n","print_predictions(tokens, pred_postags, gt_postags)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ecTpcj7-j7NT"},"source":["Explore how the trained model works on test sentences:"]},{"cell_type":"code","metadata":{"id":"ZxkfTHlxj7NT"},"source":["example_index = np.random.randint(0, len(test_data.examples))\n","\n","sentence = test_data.examples[example_index].text\n","gt_postags = test_data.examples[example_index].postag\n","tokens, pred_postags = infer_model(sentence, model, device)\n","\n","print_predictions(tokens, pred_postags, gt_postags)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9R0ta5e3j7NW"},"source":["And on our own sentences:"]},{"cell_type":"code","metadata":{"id":"z5z8BWOZj7NW"},"source":["sentence = 'I will definitely pass this homework'\n","print_predictions(*infer_model(sentence, model, device))\n","\n","sentence = 'Look , there is a bear'\n","print_predictions(*infer_model(sentence, model, device))\n","\n","sentence = 'I can not bear it anymore'\n","print_predictions(*infer_model(sentence, model, device))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CjGifM-1muHu"},"source":["## Report\n","\n","What was the most difficult thing for you in this part?\n"]},{"cell_type":"markdown","metadata":{"id":"SiAmDHG2muHv"},"source":[""]},{"cell_type":"markdown","metadata":{"id":"VqF4nRdOmuHv"},"source":["What was your history of experiments with Transformer? What hyperparameters worked best?"]},{"cell_type":"markdown","metadata":{"id":"OWgdOQk-muHv"},"source":[""]},{"cell_type":"markdown","metadata":{"id":"A3ZX_WPAmuHv"},"source":["Please attach screenshots *(tools to do it: Google Drive, GitHub issues, `IPython.display.Image`, imgur...)* of your logging facility with train and validation loss curves."]},{"cell_type":"markdown","metadata":{"id":"Lb99IUKimuHv"},"source":[""]},{"cell_type":"markdown","metadata":{"id":"h2JmwQyTj7NX"},"source":["## Checks\n","\n","* Have you managed to train the Transformer model with the test accuracy **> 85.0%**? Choose <font color='green'>YES</font> or  <font color='red'>NO</font>\n","\n","\n","* Does your Trasformer correctly tag `bear` as a `VERB` in sentence **\"I can not bear it anymore\"**? Choose <font color='green'>YES</font> or  <font color='red'>NO</font>\n","\n","\n","* Have you uploaded your `TransformerPOSTagger.pth` to Google Drive and input the checksum? Choose <font color='green'>YES</font> or  <font color='red'>NO</font>\n","\n","\n","* Have you made sure that everything runs without errors on \"Restart and Run All\" with `DO_TRAIN = False` (in particular, the model is downloaded from Drive, the test accuracy is computed)? Choose <font color='green'>YES</font> or  <font color='red'>NO</font>\n","\n","\n","* Have you made sure that everything runs without errors on \"Restart and Run All\" with `DO_TRAIN = True`? Choose <font color='green'>YES</font> or  <font color='red'>NO</font>"]},{"cell_type":"markdown","metadata":{"id":"vf3e1Jusj7NY"},"source":["## Acknowledgements\n","- Author: Karim Iskakov.\n","- Big thanks to Ben Trevett for creating the core part of the Transformer code."]}]}